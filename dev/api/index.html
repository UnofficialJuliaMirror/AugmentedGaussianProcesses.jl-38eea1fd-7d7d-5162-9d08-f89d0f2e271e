<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · AugmentedGaussianProcesses</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129106538-2', 'auto');
ga('send', 'pageview');
</script><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../assets/icon.ico" rel="icon" type="image/x-icon"/></head><body><nav class="toc"><a href="../index.html"><img class="logo" src="../assets/logo.png" alt="AugmentedGaussianProcesses logo"/></a><h1>AugmentedGaussianProcesses</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../background/">Background</a></li><li><a class="toctext" href="../userguide/">User Guide</a></li><li><a class="toctext" href="../kernel/">Kernels</a></li><li><a class="toctext" href="../examples/">Examples</a></li><li><a class="toctext" href="../comparison/">Julia GP Packages</a></li><li class="current"><a class="toctext" href>API</a><ul class="internal"><li><a class="toctext" href="#Module-1">Module</a></li><li><a class="toctext" href="#Model-Types-1">Model Types</a></li><li><a class="toctext" href="#Likelihood-Types-1">Likelihood Types</a></li><li><a class="toctext" href="#Inference-Types-1">Inference Types</a></li><li><a class="toctext" href="#Functions-and-methods-1">Functions and methods</a></li><li><a class="toctext" href="#Kernels-1">Kernels</a></li><li><a class="toctext" href="#Kernel-functions-1">Kernel functions</a></li><li><a class="toctext" href="#Prior-Means-1">Prior Means</a></li><li><a class="toctext" href="#Index-1">Index</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>API</a></li></ul><a class="edit-page" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/master/docs/src/api.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>API</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="API-Library-1" href="#API-Library-1">API Library</a></h1><hr/><ul><li><a href="#API-Library-1">API Library</a></li><ul><li><a href="#Module-1">Module</a></li><li><a href="#Model-Types-1">Model Types</a></li><li><a href="#Likelihood-Types-1">Likelihood Types</a></li><li><a href="#Inference-Types-1">Inference Types</a></li><li><a href="#Functions-and-methods-1">Functions and methods</a></li><li><a href="#Kernels-1">Kernels</a></li><li><a href="#Kernel-functions-1">Kernel functions</a></li><li><a href="#Prior-Means-1">Prior Means</a></li><li><a href="#Index-1">Index</a></li></ul></ul><h2><a class="nav-anchor" id="Module-1" href="#Module-1">Module</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.AugmentedGaussianProcesses" href="#AugmentedGaussianProcesses.AugmentedGaussianProcesses"><code>AugmentedGaussianProcesses.AugmentedGaussianProcesses</code></a> — <span class="docstring-category">Module</span>.</div><div><div><p>General Framework for the data augmented Gaussian Processes</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/AugmentedGaussianProcesses.jl#L1-L5">source</a></section><h2><a class="nav-anchor" id="Model-Types-1" href="#Model-Types-1">Model Types</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.GP" href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Class for Gaussian Processes models</p><pre><code class="language-julia">GP(X::AbstractArray{T1,N1}, y::AbstractArray{T2,N2}, kernel::Union{Kernel,AbstractVector{&lt;:Kernel}};
    noise::Real=1e-5, opt_noise::Bool=true, verbose::Int=0,
    optimizer::Bool=Adam(α=0.01),atfrequency::Int=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>noise</code> : Initial noise of the model</li><li><code>opt_noise</code> : Flag for optimizing the noise σ=Σ(y-f)^2/N</li><li><code>mean</code> : Option for putting a prior mean</li><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimizer</code> : Optimizer for kernel hyperparameters (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>)</li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior-1"><code>MeanPrior</code></a></li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/models/GP.jl#L1-L30">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.VGP" href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Class for variational Gaussian Processes models (non-sparse)</p><pre><code class="language-julia">VGP(X::AbstractArray{T1,N1},y::AbstractArray{T2,N2},kernel::Union{Kernel,AbstractVector{&lt;:Kernel}},
    likelihood::LikelihoodType,inference::InferenceType;
    verbose::Int=0,optimizer::Union{Bool,Optimizer,Nothing}=Adam(α=0.01),atfrequency::Integer=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li><li><code>likelihood</code> : likelihood of the model, currently implemented : Gaussian, Bernoulli (with logistic link), Multiclass (softmax or logistic-softmax) see <a href="../userguide/#likelihood_user-1"><code>Likelihood Types</code></a></li><li><code>inference</code> : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the <a href="../userguide/#compat_table-1"><code>Compatibility Table</code></a></li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimizer</code> : Optimizer for kernel hyperparameters (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>)</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior-1"><code>MeanPrior</code></a></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/models/VGP.jl#L1-L30">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.SVGP" href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Class for sparse variational Gaussian Processes</p><pre><code class="language-julia">SVGP(X::AbstractArray{T1},y::AbstractArray{T2},kernel::Union{Kernel,AbstractVector{&lt;:Kernel}},
    likelihood::LikelihoodType,inference::InferenceType, nInducingPoints::Int;
    verbose::Int=0,optimizer::Union{Optimizer,Nothing,Bool}=Adam(α=0.01),atfrequency::Int=1,
    mean::Union{&lt;:Real,AbstractVector{&lt;:Real},PriorMean}=ZeroMean(),
    IndependentPriors::Bool=true,Zoptimizer::Union{Optimizer,Nothing,Bool}=false,
    ArrayType::UnionAll=Vector)</code></pre><p>Argument list :</p><p><strong>Mandatory arguments</strong></p><ul><li><code>X</code> : input features, should be a matrix N×D where N is the number of observation and D the number of dimension</li><li><code>y</code> : input labels, can be either a vector of labels for multiclass and single output or a matrix for multi-outputs (note that only one likelihood can be applied)</li><li><code>kernel</code> : covariance function, can be either a single kernel or a collection of kernels for multiclass and multi-outputs models</li><li><code>likelihood</code> : likelihood of the model, currently implemented : Gaussian, Student-T, Laplace, Bernoulli (with logistic link), Bayesian SVM, Multiclass (softmax or logistic-softmax) see <a href="../userguide/#likelihood_user-1"><code>Likelihood</code></a></li><li><code>inference</code> : inference for the model, can be analytic, numerical or by sampling, check the model documentation to know what is available for your likelihood see the <a href="../userguide/#compat_table-1"><code>Compatibility table</code></a></li><li><code>nInducingPoints</code> : number of inducing points</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>verbose</code> : How much does the model print (0:nothing, 1:very basic, 2:medium, 3:everything)</li><li><code>optimizer</code> : Optimizer for kernel hyperparameters (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>)</li><li><code>atfrequency</code> : Choose how many variational parameters iterations are between hyperparameters optimization</li><li><code>mean</code> : PriorMean object, check the documentation on it <a href="../userguide/#meanprior-1"><code>MeanPrior</code></a></li><li><code>IndependentPriors</code> : Flag for setting independent or shared parameters among latent GPs</li><li><code>optimizer</code> : Optimizer for inducing point locations (to be selected from <a href="https://github.com/jacobcvt12/GradDescent.jl">GradDescent.jl</a>)</li><li><code>ArrayType</code> : Option for using different type of array for storage (allow for GPU usage)</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/models/SVGP.jl#L1-L30">source</a></section><h2><a class="nav-anchor" id="Likelihood-Types-1" href="#Likelihood-Types-1">Likelihood Types</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.GaussianLikelihood" href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>Gaussian Likelihood</strong></p><p>Classical Gaussian noise : <span>$p(y|f) = \mathcal{N}(y|f,\epsilon)$</span></p><pre><code class="language-julia">GaussianLikelihood(ϵ::T=1e-3) #ϵ is the variance</code></pre><p>There is no augmentation needed for this likelihood which is already conjugate</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/gaussian.jl#L1-L11">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.StudentTLikelihood" href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>Student-T likelihood</strong></p><p>Student-t likelihood for regression: <span>$\frac{\Gamma((\nu+1)/2)}{\sqrt{\nu\pi}\sigma\Gamma(\nu/2)}\left(1+(y-f)^2/(\sigma^2\nu)\right)^{(-(\nu+1)/2)}$</span> see <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">wiki page</a></p><pre><code class="language-julia">StudentTLikelihood(ν::T,σ::Real=one(T)) #ν is the number of degrees of freedom
#σ is the variance for local scale of the data.</code></pre><hr/><p>For the analytical solution, it is augmented via:</p><div>\[p(y|f,\omega) = \mathcal{N}(y|f,\sigma^2\omega)\]</div><p>Where <span>$\omega \sim \mathcal{IG}(\frac{\nu}{2},\frac{\nu}{2})$</span> where <span>$\mathcal{IG}$</span> is the inverse gamma distribution See paper <a href="http://www.jmlr.org/papers/volume12/jylanki11a/jylanki11a.pdf">Robust Gaussian Process Regression with a Student-t Likelihood</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/studentt.jl#L1-L20">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.LaplaceLikelihood" href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>Laplace likelihood</strong></p><p>Laplace likelihood for regression: <span>$\frac{1}{2\beta}\exp\left(-\frac{|y-f|}{\beta}\right)$</span> see <a href="https://en.wikipedia.org/wiki/Laplace_distribution">wiki page</a></p><pre><code class="language-julia">LaplaceLikelihood(β::T=1.0)  #  Laplace likelihood with scale β</code></pre><hr/><p>For the analytical solution, it is augmented via:</p><div>\[p(y|f,\omega) = \mathcal{N}(y|f,\omega^{-1})\]</div><p>where <span>$\omega \sim \text{Exp}\left(\omega \mid \frac{1}{2 \beta^2}\right)$</span>, and Exp is the <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential distribution</a> We approximate ``q(\omega) = \mathcal{GIG}\left(\omega \mid a,b,p\right)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/laplace.jl#L1-L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticLikelihood" href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>Logistic Likelihood</strong></p><p>Bernoulli likelihood with a logistic link for the Bernoulli likelihood     <span>$p(y|f) = \sigma(yf) = \frac{1}{1+\exp(-yf)}$</span>, (for more info see : <a href="https://en.wikipedia.org/wiki/Logistic_function">wiki page</a>)</p><pre><code class="language-julia">LogisticLikelihood()</code></pre><hr/><p>For the analytic version the likelihood, it is augmented via:</p><div>\[p(y|f,\omega) = \exp\left(\frac{1}{2}\left(yf - (yf)^2 \omega\right)\right)\]</div><p>where <span>$\omega \sim \text{PG}(\omega\mid 1, 0)$</span>, and PG is the Polya-Gamma distribution See paper : <a href="https://arxiv.org/abs/1802.06383">Efficient Gaussian Process Classification Using Polya-Gamma Data Augmentation</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/logistic.jl#L1-L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.HeteroscedasticLikelihood" href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>Heteroscedastic Likelihood</strong></p><p>Gaussian with heteroscedastic noise given by another gp: <span>$p(y|f,g) = \mathcal{N}(y|f,(\lambda\sigma(g))^{-1})$</span></p><pre><code class="language-julia">HeteroscedasticLikelihood([kernel=RBFKernel(),[priormean=0.0]])</code></pre><p>Augmentation is described here (#TODO)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/heteroscedastic.jl#L1-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.BayesianSVM" href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>Bayesian SVM</strong></p><p>The <a href="https://arxiv.org/abs/1707.05532">Bayesian SVM</a> is a Bayesian interpretation of the classical SVM. <span>$p(y|f) \propto \exp\left(2\max(1-yf,0)\right)$</span></p><pre><code class="language-julia">BayesianSVM()</code></pre><hr/><p>For the analytic version of the likelihood, it is augmented via:</p><div>\[p(y|f,\omega) = \frac{1}{\sqrt{2\pi\omega}}\exp\left(-\frac{1}{2}\frac{(1+\omega-yf)^2}{\omega}\right)\]</div><p>where <span>$\omega\sim 1_{[0,\infty]}$</span> has an improper prior (his posterior is however has a valid distribution (Generalized Inverse Gaussian)). For reference <a href="http://ecmlpkdd2017.ijs.si/papers/paperID502.pdf">see this paper</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/bayesiansvm.jl#L1-L16">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.SoftMaxLikelihood" href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>SoftMax Likelihood</strong></p><p>Multiclass likelihood with Softmax transformation: <span>$p(y=i|{f_k}) = \exp(f_i)/ \sum_{j=1}\exp(f_j)$</span></p><p>There is no possible augmentation for this likelihood</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/softmax.jl#L1-L7">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.LogisticSoftMaxLikelihood" href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>The Logistic-Softmax likelihood</strong></p><p>The multiclass likelihood with a logistic-softmax mapping: : <span>$p(y=i|\{f_k\}) = \sigma(f_i)/ \sum_k \sigma(f_k)$</span> where σ is the logistic function has the same properties as softmax.</p><hr/><p>For the analytical version, the likelihood is augmented multiple times to obtain :</p><div>\[#TODO\]</div><p>Paper with details under submission</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/logisticsoftmax.jl#L1-L14">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.PoissonLikelihood" href="#AugmentedGaussianProcesses.PoissonLikelihood"><code>AugmentedGaussianProcesses.PoissonLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Poisson Likelihood</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/poisson.jl#L1">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.NegBinomialLikelihood" href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>NegBinomial Likelihood</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/likelihood/negativebinomial.jl#L1">source</a></section><h2><a class="nav-anchor" id="Inference-Types-1" href="#Inference-Types-1">Inference Types</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticVI" href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>AnalyticVI</strong></p><p>Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation) All data is used at each iteration (use AnalyticSVI for Stochastic updates)</p><pre><code class="language-julia">AnalyticVI(;ϵ::T=1e-5)</code></pre><p><strong>Keywords arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/inference/analyticVI.jl#L1-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.AnalyticSVI" href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><strong>AnalyticSVI</strong> Stochastic Variational Inference solver for conjugate or conditionally conjugate likelihoods (non-gaussian are made conjugate via augmentation)</p><pre><code class="language-julia">AnalyticSVI(nMinibatch::Integer;ϵ::T=1e-5,optimizer::Optimizer=InverseDecay())</code></pre><pre><code class="language-none">- `nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keywords arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `InverseDecay()` (ρ=(τ+iter)^-κ)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/inference/analyticVI.jl#L42-L55">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.GibbsSampling" href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>GibbsSampling</strong></p><p>Draw samples from the true posterior via Gibbs Sampling.</p><pre><code class="language-julia">GibbsSampling(;ϵ::T=1e-5,nBurnin::Int=100,samplefrequency::Int=10)</code></pre><p><strong>Keywords arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria
- `nBurnin::Int` : Number of samples discarded before starting to save samples
- `samplefrequency::Int` : Frequency of sampling</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/inference/gibbssampling.jl#L1-L14">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureVI" href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>QuadratureVI</strong></p><p>Variational Inference solver by approximating gradients via numerical integration via Quadrature</p><pre><code class="language-julia">QuadratureVI(ϵ::T=1e-5,nGaussHermite::Integer=20,optimizer::Optimizer=Momentum(η=0.0001))</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria
- `nGaussHermite::Int` : Number of points for the integral estimation
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `Momentum(η=0.0001)`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/inference/quadratureVI.jl#L1-L15">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.QuadratureSVI" href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><strong>QuadratureSVI</strong></p><p>Stochastic Variational Inference solver by approximating gradients via numerical integration via Quadrature</p><pre><code class="language-julia">QuadratureSVI(nMinibatch::Integer;ϵ::T=1e-5,nGaussHermite::Integer=20,optimizer::Optimizer=Adam(α=0.1))</code></pre><pre><code class="language-none">-`nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria, which can be user defined
- `nGaussHermite::Int` : Number of points for the integral estimation (for the QuadratureVI)
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl](https://github.com/jacobcvt12/GradDescent.jl) package. Default is `Momentum(η=0.001)`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/inference/quadratureVI.jl#L47-L62">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationVI" href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><code>MCIntegrationVI(;ϵ::T=1e-5,nMC::Integer=1000,optimizer::Optimizer=Adam(α=0.1))</code></p><p>Constructor for Variational Inference via MC Integration approximation.</p><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria, which can be user defined
- `nMC::Int` : Number of samples per data point for the integral evaluation
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl]() package. Default is `Adam()`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/inference/MCVI.jl#L23-L33">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.MCIntegrationSVI" href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>MCIntegrationSVI(;ϵ::T=1e-5,nMC::Integer=1000,optimizer::Optimizer=Adam(α=0.1))</code></p><p>Constructor for Stochastic Variational Inference via MC integration approximation.</p><p><strong>Argument</strong></p><pre><code class="language-none">-`nMinibatch::Integer` : Number of samples per mini-batches</code></pre><p><strong>Keyword arguments</strong></p><pre><code class="language-none">- `ϵ::T` : convergence criteria, which can be user defined
- `nMC::Int` : Number of samples per data point for the integral evaluation
- `optimizer::Optimizer` : Optimizer used for the variational updates. Should be an Optimizer object from the [GradDescent.jl]() package. Default is `Adam()`</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/inference/MCVI.jl#L37-L51">source</a></section><h2><a class="nav-anchor" id="Functions-and-methods-1" href="#Functions-and-methods-1">Functions and methods</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.train!" href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>train!(model::AbstractGP;iterations::Integer=100,callback=0,conv_function=0)</code></p><p>Function to train the given GP <code>model</code>.</p><p><strong>Keyword Arguments</strong></p><p>there are options to change the number of max iterations,</p><ul><li><code>iterations::Int</code> : Number of iterations (not necessarily epochs!)for training</li><li><code>callback::Function</code> : Callback function called at every iteration. Should be of type <code>function(model,iter) ...  end</code></li><li><code>conv_function::Function</code> : Convergence function to be called every iteration, should return a scalar and take the same arguments as <code>callback</code></li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/training.jl#L2-L13">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.predict_f" href="#AugmentedGaussianProcesses.predict_f"><code>AugmentedGaussianProcesses.predict_f</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Compute the mean of the predicted latent distribution of <code>f</code> on <code>X_test</code> for the variational GP <code>model</code></p><p>Return also the variance if <code>covf=true</code> and the full covariance if <code>fullcov=true</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/predictions.jl#L22-L26">source</a><div><div><p>Compute the mean of the predicted latent distribution of f on <code>X_test</code> for a sparse GP <code>model</code> Return also the variance if <code>covf=true</code> and the full covariance if <code>fullcov=true</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/predictions.jl#L45-L48">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.predict_y" href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>predict_y(model::AbstractGP{T,&lt;:RegressionLikelihood},X_test::AbstractMatrix)</code></p><p>Return the predictive mean of <code>X_test</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/predictions.jl#L93-L97">source</a><div><div><p><code>predict_y(model::AbstractGP{T,&lt;:ClassificationLikelihood},X_test::AbstractMatrix)</code></p><p>Return the predicted most probable sign of <code>X_test</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/predictions.jl#L102-L106">source</a><div><div><p><code>predict_y(model::AbstractGP{T,&lt;:MultiClassLikelihood},X_test::AbstractMatrix)</code></p><p>Return the predicted most probable class of <code>X_test</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/predictions.jl#L111-L115">source</a><div><div><p><code>predict_y(model::AbstractGP{T,&lt;:EventLikelihood},X_test::AbstractMatrix)</code></p><p>Return the expected number of events for the locations <code>X_test</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/predictions.jl#L122-L126">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.proba_y" href="#AugmentedGaussianProcesses.proba_y"><code>AugmentedGaussianProcesses.proba_y</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p><code>proba_y(model::AbstractGP,X_test::AbstractMatrix)</code></p><p>Return the probability distribution p(y<em>test|model,X</em>test) :</p><pre><code class="language-none">- Tuple of vectors of mean and variance for regression
- Vector of probabilities of y_test = 1 for binary classification
- Dataframe with columns and probability per class for multi-class classification</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/predictions.jl#L138-L146">source</a></section><h2><a class="nav-anchor" id="Kernels-1" href="#Kernels-1">Kernels</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.KernelModule.RBFKernel" href="#AugmentedGaussianProcesses.KernelModule.RBFKernel"><code>AugmentedGaussianProcesses.KernelModule.RBFKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Radial Basis Function Kernel also called RBF or SE(Squared Exponential)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/RBF.jl#L1">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.KernelModule.MaternKernel" href="#AugmentedGaussianProcesses.KernelModule.MaternKernel"><code>AugmentedGaussianProcesses.KernelModule.MaternKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Matern Kernel</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/matern.jl#L1-L3">source</a></section><h2><a class="nav-anchor" id="Kernel-functions-1" href="#Kernel-functions-1">Kernel functions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.KernelModule.kernelmatrix" href="#AugmentedGaussianProcesses.KernelModule.kernelmatrix"><code>AugmentedGaussianProcesses.KernelModule.kernelmatrix</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Create the covariance matrix between the matrix X1 and X2 with the covariance function <code>kernel</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/kernelmatrix.jl#L1">source</a><div><div><p>Compute the covariance matrix of the matrix X, optionally only compute the diagonal terms</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/kernelmatrix.jl#L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.KernelModule.kernelmatrix!" href="#AugmentedGaussianProcesses.KernelModule.kernelmatrix!"><code>AugmentedGaussianProcesses.KernelModule.kernelmatrix!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Compute the covariance matrix between the matrix X1 and X2 with the covariance function <code>kernel</code> in preallocated matrix K</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/kernelmatrix.jl#L8">source</a><div><div><p>Compute the covariance matrix of the matrix X in preallocated matrix K, optionally only compute the diagonal terms</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/kernelmatrix.jl#L30">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.KernelModule.getvariance" href="#AugmentedGaussianProcesses.KernelModule.getvariance"><code>AugmentedGaussianProcesses.KernelModule.getvariance</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Return the variance of the kernel</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/kernel.jl#L29">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.KernelModule.getlengthscales" href="#AugmentedGaussianProcesses.KernelModule.getlengthscales"><code>AugmentedGaussianProcesses.KernelModule.getlengthscales</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Return the lengthscale of the IsoKernel</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/kernel.jl#L34">source</a><div><div><p>Return the lengthscales of the ARD Kernel</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/kernels/kernel.jl#L39">source</a></section><h2><a class="nav-anchor" id="Prior-Means-1" href="#Prior-Means-1">Prior Means</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.ZeroMean" href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>ZeroMean</p><pre><code class="language-julia">ZeroMean()</code></pre><p>Construct a mean prior set to 0 and cannot be changed.</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/prior/zeromean.jl#L4-L10">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.ConstantMean" href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>ConstantMean</strong></p><pre><code class="language-julia">ConstantMean(c::T=1.0;opt::Optimizer=Adam(α=0.01))</code></pre><p>Construct a prior mean with constant <code>c</code> Optionally set an optimizer <code>opt</code> (<code>Adam(α=0.01)</code> by default)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/prior/constantmean.jl#L6-L14">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="AugmentedGaussianProcesses.EmpiricalMean" href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p><strong>EmpiricalMean</strong> <code>julia` function EmpiricalMean(c::V=1.0;opt::Optimizer=Adam(α=0.01)) where {V&lt;:AbstractVector{&lt;:Real}}</code> Construct a constant mean with values <code>c</code> Optionally give an optimizer <code>opt</code> (<code>Adam(α=0.01)</code> by default)</p></div></div><a class="source-link" target="_blank" href="https://github.com/theogf/AugmentedGaussianProcesses.jl/blob/e30e355fd57a7c55cc21116247ab1668ef31ff69/src/prior/empiricalmean.jl#L6-L13">source</a></section><h2><a class="nav-anchor" id="Index-1" href="#Index-1">Index</a></h2><ul><li><a href="#AugmentedGaussianProcesses.AnalyticVI"><code>AugmentedGaussianProcesses.AnalyticVI</code></a></li><li><a href="#AugmentedGaussianProcesses.BayesianSVM"><code>AugmentedGaussianProcesses.BayesianSVM</code></a></li><li><a href="#AugmentedGaussianProcesses.ConstantMean"><code>AugmentedGaussianProcesses.ConstantMean</code></a></li><li><a href="#AugmentedGaussianProcesses.EmpiricalMean"><code>AugmentedGaussianProcesses.EmpiricalMean</code></a></li><li><a href="#AugmentedGaussianProcesses.GP"><code>AugmentedGaussianProcesses.GP</code></a></li><li><a href="#AugmentedGaussianProcesses.GaussianLikelihood"><code>AugmentedGaussianProcesses.GaussianLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.GibbsSampling"><code>AugmentedGaussianProcesses.GibbsSampling</code></a></li><li><a href="#AugmentedGaussianProcesses.HeteroscedasticLikelihood"><code>AugmentedGaussianProcesses.HeteroscedasticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.KernelModule.MaternKernel"><code>AugmentedGaussianProcesses.KernelModule.MaternKernel</code></a></li><li><a href="#AugmentedGaussianProcesses.KernelModule.RBFKernel"><code>AugmentedGaussianProcesses.KernelModule.RBFKernel</code></a></li><li><a href="#AugmentedGaussianProcesses.LaplaceLikelihood"><code>AugmentedGaussianProcesses.LaplaceLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticLikelihood"><code>AugmentedGaussianProcesses.LogisticLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.LogisticSoftMaxLikelihood"><code>AugmentedGaussianProcesses.LogisticSoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationVI"><code>AugmentedGaussianProcesses.MCIntegrationVI</code></a></li><li><a href="#AugmentedGaussianProcesses.NegBinomialLikelihood"><code>AugmentedGaussianProcesses.NegBinomialLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.PoissonLikelihood"><code>AugmentedGaussianProcesses.PoissonLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureVI"><code>AugmentedGaussianProcesses.QuadratureVI</code></a></li><li><a href="#AugmentedGaussianProcesses.SVGP"><code>AugmentedGaussianProcesses.SVGP</code></a></li><li><a href="#AugmentedGaussianProcesses.SoftMaxLikelihood"><code>AugmentedGaussianProcesses.SoftMaxLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.StudentTLikelihood"><code>AugmentedGaussianProcesses.StudentTLikelihood</code></a></li><li><a href="#AugmentedGaussianProcesses.VGP"><code>AugmentedGaussianProcesses.VGP</code></a></li><li><a href="#AugmentedGaussianProcesses.ZeroMean"><code>AugmentedGaussianProcesses.ZeroMean</code></a></li><li><a href="#AugmentedGaussianProcesses.AnalyticSVI"><code>AugmentedGaussianProcesses.AnalyticSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.KernelModule.getlengthscales"><code>AugmentedGaussianProcesses.KernelModule.getlengthscales</code></a></li><li><a href="#AugmentedGaussianProcesses.KernelModule.getvariance"><code>AugmentedGaussianProcesses.KernelModule.getvariance</code></a></li><li><a href="#AugmentedGaussianProcesses.KernelModule.kernelmatrix"><code>AugmentedGaussianProcesses.KernelModule.kernelmatrix</code></a></li><li><a href="#AugmentedGaussianProcesses.KernelModule.kernelmatrix!"><code>AugmentedGaussianProcesses.KernelModule.kernelmatrix!</code></a></li><li><a href="#AugmentedGaussianProcesses.MCIntegrationSVI"><code>AugmentedGaussianProcesses.MCIntegrationSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.QuadratureSVI"><code>AugmentedGaussianProcesses.QuadratureSVI</code></a></li><li><a href="#AugmentedGaussianProcesses.predict_f"><code>AugmentedGaussianProcesses.predict_f</code></a></li><li><a href="#AugmentedGaussianProcesses.predict_y"><code>AugmentedGaussianProcesses.predict_y</code></a></li><li><a href="#AugmentedGaussianProcesses.proba_y"><code>AugmentedGaussianProcesses.proba_y</code></a></li><li><a href="#AugmentedGaussianProcesses.train!"><code>AugmentedGaussianProcesses.train!</code></a></li></ul><footer><hr/><a class="previous" href="../comparison/"><span class="direction">Previous</span><span class="title">Julia GP Packages</span></a></footer></article></body></html>
